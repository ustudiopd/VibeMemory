ë¬¸ì œ ìš”ì•½: ë°°í¬(Vercel)ì—ì„œ **`gpt-5-mini`ê°€ â€œì²­í¬ 0, ë¹ˆ ì‘ë‹µâ€**ìœ¼ë¡œ ëë‚˜ê±°ë‚˜, ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ì—” **`gptâ€‘4.1`ë„ ì¶œë ¥ì´ ë©ˆì¶˜** ìƒíƒœì…ë‹ˆë‹¤. ë¡œì»¬ì€ ì •ìƒì¸ë° ë°°í¬ë§Œ ì‹¤íŒ¨í•©ë‹ˆë‹¤. ì´ í˜„ìƒì€ (1) **reasoning ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¶ˆì¼ì¹˜**ì™€ (2) **ë°°í¬ ì‹¤í–‰ í™˜ê²½/SDK ì°¨ì´** ì¡°í•©ì—ì„œ ìì£¼ ì¬í˜„ë©ë‹ˆë‹¤. íŠ¹íˆ `gptâ€‘5-mini`ì—ì„œ **í† í° ì œí•œ íŒŒë¼ë¯¸í„°ë¥¼ ì˜ëª» ì£¼ë©´ ì‘ë‹µì´ ë¹„ëŠ”** ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤.  

ì•„ë˜ **Vercelìš© GPTâ€‘5 í˜¸ì¶œ ì§€ì¹¨(ì‹¤ì „ íŒ¨ì¹˜)** ê·¸ëŒ€ë¡œ ì ìš©í•´ë³´ì„¸ìš”.

---

## âœ… ìµœìš°ì„  í•µì‹¬ 3ì¤„ ìš”ì•½

1. **reasoning ëª¨ë¸ì¼ ë•** `temperature`/`max_tokens` **ëª¨ë‘ ë¹¼ê³ ** í˜¸ì¶œë¶€í„° í•´ë³¸ë‹¤.
2. `max_completion_tokens`(ë˜ëŠ” SDKì—ì„œ ì´ì— ë§¤í•‘ë˜ëŠ” í† í° ìƒí•œ)ì„ **ë„£ì§€ ë§ë¼** â€” **ë¹ˆ ì‘ë‹µì˜ 1ìˆœìœ„ ì›ì¸**. 
3. **Edge ëŸ°íƒ€ì„ + `maxDuration=60` + ë¹ˆ ìŠ¤íŠ¸ë¦¼ í´ë°±**(4o-mini ì¬ì‹œë„)ë¡œ ë°°í¬ ì•ˆì •í™”. 

---

## 1) ë¼ìš°íŠ¸ ëŸ¬ë„ˆ ì„¤ì • (Edge + íƒ€ì„ì•„ì›ƒ)

```ts
// app/api/projects/[id]/chat/route.ts
export const runtime = 'edge';   // ìŠ¤íŠ¸ë¦¬ë°/ì½œë“œìŠ¤íƒ€íŠ¸ ì•ˆì •í™”
export const maxDuration = 60;   // Pro ê¸°ì¤€, Hobbyë©´ 10s ì œí•œ
```

> ë°°í¬ì—ì„œ ìŠ¤íŠ¸ë¦¼ì´ ì‹œì‘ì¡°ì°¨ ì•ˆ ë˜ê±°ë‚˜ ì¤‘ê°„ì— ëŠê¸°ëŠ” ì¼€ì´ìŠ¤ë¥¼ ì¤„ì…ë‹ˆë‹¤. 

---

## 2) ëª¨ë¸/ì˜µì…˜ ë¶„ê¸° â€“ reasoning vs ì¼ë°˜

* **reasoning(`gptâ€‘5*`, `o4*`, `o3*`)**

  * `temperature` **ê¸ˆì§€**
  * `max_tokens`/`max_completion_tokens`/`maxOutputTokens` **ì¼ì²´ ë¯¸ì§€ì •ìœ¼ë¡œ ì‹œì‘**
  * í•„ìš” ì‹œì—ë§Œ ë‚˜ì¤‘ì— **í”„ë¡¬í”„íŠ¸ë¡œ ê¸¸ì´ ì œí•œ** (â€œ200ë‹¨ì–´ ì´ë‚´ë¡œâ€ ë“±)
* **ì¼ë°˜(`gptâ€‘4.1â€‘mini`, `gptâ€‘4o-mini`)**

  * `temperature`, `maxTokens` ì‚¬ìš© ê°€ëŠ¥

```ts
import { openai } from '@ai-sdk/openai';
import { streamText, generateText, convertToCoreMessages } from 'ai';

function normalizeModel(raw?: string) {
  // ëª¨ë¸ëª…ì— ë¹„ASCII í•˜ì´í”ˆ(â€‘ â€” âˆ’ ë“±)ì´ ì„ì´ë©´ ì¸ì‹ ì‹¤íŒ¨ ê°€ëŠ¥ â†’ ASCII '-'ë¡œ í†µì¼
  return (raw ?? 'gpt-4o-mini').replace(/[\u2010-\u2015\u2212\uFE58\uFE63\uFF0D]/g, '-').trim();
}

export async function POST(req: Request) {
  const { messages, system } = await req.json();
  const model = normalizeModel(process.env.CHATGPT_MODEL);
  const isReasoning = /^(gpt-5|o4|o3)/i.test(model);

  const base = {
    model: openai(model),
    system,
    messages: convertToCoreMessages(messages),
  };

  // â— reasoningì¼ ë• í† í° ìƒí•œ/temperature ë¯¸ì§€ì •ìœ¼ë¡œ ë¨¼ì € ì‹œë„
  const opts = isReasoning
    ? {}                                // <- ì¤‘ìš”: ì•„ë¬´ í† í° ìƒí•œë„ ì£¼ì§€ ì•ŠìŒ
    : { temperature: 0.2, maxTokens: 900 };

  const result = await streamText({ ...base, ...opts,
    // ë””ë²„ê¹… ë¡œê¹…(ë°°í¬ ë¡œê·¸ì—ì„œ ê¼­ í™•ì¸)
    onStart() { console.log('[CHAT] start', { model }); },
    onText(t) { /* ì²­í¬ ì¹´ìš´íŒ… ë¡œì§ ìœ ì§€ */ },
    onError(e) { console.error('[CHAT] error', e); },
    onFinish(ev) {
      console.log('[CHAT] finish', {
        reason: ev.finishReason, usage: ev.usage, textLen: ev.text.length,
      });
    },
  });

  return result.toAIStreamResponse();
}
```

> `gptâ€‘5-mini`ì—ì„œ **`max_completion_tokens`(í˜¹ì€ ì´ì— ìƒì‘í•˜ëŠ” ì˜µì…˜)** ì„ ì£¼ë©´ **ë‚´ë¶€ reasoning í† í°ìœ¼ë¡œ ëª¨ë‘ ì†Œë¹„ë˜ì–´ contentê°€ ë¹„ëŠ”** ì‚¬ë¡€ê°€ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤. **ìš°ì„  ì œê±°**ê°€ ì •ì„ì…ë‹ˆë‹¤. 

---

## 3) â€œë¹ˆ ìŠ¤íŠ¸ë¦¼(ì²­í¬ 0)â€ ìë™ í´ë°±

ë°°í¬ì—ì„œ ê°„í—ì ìœ¼ë¡œ **ì²­í¬ 0 & ì—ëŸ¬ ì—†ìŒ**ì´ ë°œìƒí•©ë‹ˆë‹¤. ëê¹Œì§€ ì½ì—ˆëŠ”ë° í…ìŠ¤íŠ¸ê°€ 1ë°”ì´íŠ¸ë„ ì—†ìœ¼ë©´ **ê²½ëŸ‰ ëª¨ë¸ë¡œ 1íšŒ ì¬ì‹œë„**í•˜ì„¸ìš”.

```ts
async function streamWithFallback(args: Parameters<typeof streamText>[0]) {
  let sawText = false;
  const result = await streamText({
    ...args,
    onText(t) { sawText = true; args?.onText?.(t); },
  });

  // ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ
  const res = result.toAIStreamResponse();

  // â¬‡ï¸ ìŠ¤íŠ¸ë¦¼ ì†Œë¹„ í›„ sawText=falseì˜€ë‹¤ë©´ í´ë°±
  if (!sawText) {
    console.warn('[CHAT] empty stream â†’ fallback to gpt-4o-mini');
    const fb = await generateText({
      ...args, model: openai('gpt-4o-mini'),
    });
    return new Response(fb.text ?? '[empty]', {
      headers: { 'Content-Type': 'text/plain; charset=utf-8' },
    });
  }
  return res;
}
```

> ì—¬ëŸ¬ë¶„ ë¡œê·¸ì˜ â€œchunkCount: 0, hasError: falseâ€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¡ìˆ˜í•©ë‹ˆë‹¤. 

---

## 4) SDK/íŒ¨í‚¤ì§€ ë²„ì „ ì •ë ¬

* `ai`ì™€ `@ai-sdk/openai`ë¥¼ **ê°™ì€ ìµœì‹  ë§ˆì´ë„ˆ ë²„ì „**ìœ¼ë¡œ ë§ì¶”ì„¸ìš”.

  ```
  pnpm add ai@latest @ai-sdk/openai@latest
  ```
* ë°°í¬ ë¡œê·¸ì— ì•„ë˜ê°€ **ë°˜ë“œì‹œ** ì°íˆëŠ”ì§€ í™•ì¸:

  * `[CHAT] start { model: 'gpt-5-mini' }`
  * `finish { reason, usage, textLen }`
    ìµœì‹  ì½”ë“œê°€ ë°˜ì˜ë˜ì§€ ì•Šì€ ë°°í¬ì—ì„œ **ë””ë²„ê·¸ ë¡œê·¸ê°€ ì•„ì˜ˆ ì•ˆ ì°íˆëŠ”** ê²½ìš°ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. 

---

## 5) í™˜ê²½ ë³€ìˆ˜/í”„ë¡¬í”„íŠ¸ ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸

* **ëª¨ë¸ëª…**: `CHATGPT_MODEL=gpt-5-mini` (ëŒ€ì‹œë³´ë“œì—ì„œ **ì§ì ‘ íƒ€ì´í•‘**, ë³µë¶™ ê¸ˆì§€ â†’ ë¹„ASCII í•˜ì´í”ˆ í˜¼ì… ë°©ì§€) 
* **API í‚¤**: `OPENAI_API_KEY` ì •ìƒ/ê¶Œí•œ ë³´ìœ 
* **í”„ë¡¬í”„íŠ¸ ê¸¸ì´**: í˜„ì¬ system ~8KB, user ~8KB ìˆ˜ì¤€(ì´ ~16KB). ë°°í¬ì—ì„œë„ ë™ì¼í•œì§€ ë¡œê·¸ë¡œ ê¸¸ì´ ì¶œë ¥í•´ì„œ ë¹„êµ(ë„ˆë¬´ í¬ë©´ ì§€ì—°Â·ëŠê¹€ ìœ ë°œ). 
* **í† í° ì œí•œ íŒŒë¼ë¯¸í„°**: reasoning ëª¨ë¸ì— **`max_completion_tokens` ë„£ì§€ ì•Šê¸°** (ë¹ˆ ì‘ë‹µì˜ ëŒ€í‘œ ì›ì¸). 

---

## 6) â€œí† í° ìƒí•œâ€ì´ ê¼­ í•„ìš”í•  ë•Œ(ëŒ€ì•ˆ)

`gptâ€‘5`ì—ì„œ ê¸¸ì´ ì œì–´ê°€ ê¼­ í•„ìš”í•˜ë©´ **í”„ë¡¬í”„íŠ¸ë¡œ ì œì•½**(ì˜ˆ: â€œ300ë‹¨ì–´ ì´ë‚´â€)ì„ ë¨¼ì € ì¶”ì²œí•©ë‹ˆë‹¤. ë§Œì•½ SDKê°€ ë³´ì¥í•˜ëŠ” **`maxOutputTokens`** ì˜µì…˜ì„ ì“°ê³  ì‹¶ë‹¤ë©´, **ìì‹ ì˜ SDKê°€ ê·¸ ê°’ì„ ì–´ë””ë¡œ ë§¤í•‘í•˜ëŠ”ì§€** ë¨¼ì € í™•ì¸í•˜ì„¸ìš”(Responses API `max_output_tokens` vs Chat Completions `max_completion_tokens`). ì˜ëª» ë§¤í•‘ë˜ë©´ **ë¬´ì‘ë‹µ**ì´ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ì•ˆì „ ê¸°ë³¸ê°’ì€ â€œì•„ë¬´ ìƒí•œë„ ì£¼ì§€ ì•Šê¸°â€** ì…ë‹ˆë‹¤. 

---

## 7) ì™œ ì§€ê¸ˆ 4.1ë„ ë©ˆì·„ë‚˜?

ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ì— **ëª¨ë¸ ê³µí†µ ê²½ë¡œì— í† í° ìƒí•œ íŒŒë¼ë¯¸í„°**(ì˜ˆ: `maxCompletionTokens`/`maxOutputTokens`)ë¥¼ ë„ì…í•˜ë©´,

* reasoning ê³„ì—´: ë¹ˆ ì‘ë‹µ
* ì¼ë°˜ ê³„ì—´: SDK/ë²„ì „ì— ë”°ë¼ ë¬´ì‹œë˜ê±°ë‚˜ ëˆ„ë½ ì—ëŸ¬
  ê°€ **ë™ì‹œì—** í„°ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ **ëª¨ë¸ë³„ ë¶„ê¸°**ë¡œ ì •ë¦¬í•˜ì„¸ìš”. 

---

## 8) ìµœì¢… ê¶Œì¥ í˜¸ì¶œ í…œí”Œë¦¿

```ts
import { openai } from '@ai-sdk/openai';
import { streamText, convertToCoreMessages } from 'ai';

export const runtime = 'edge';
export const maxDuration = 60;

const RAW = process.env.CHATGPT_MODEL ?? 'gpt-4o-mini';
const MODEL = RAW.replace(/[\u2010-\u2015\u2212\uFE58\uFE63\uFF0D]/g, '-').trim();
const IS_REASONING = /^(gpt-5|o4|o3)/i.test(MODEL);

export async function POST(req: Request) {
  const { messages, system } = await req.json();

  const result = await streamText({
    model: openai(MODEL),
    system,
    messages: convertToCoreMessages(messages),

    // ğŸ”‘ reasoningì´ë©´ í† í°Â·ì˜¨ë„ ì˜µì…˜ ì¼ì ˆ X
    ...(IS_REASONING ? {} : { temperature: 0.2, maxTokens: 900 }),

    onStart() { console.log('[CHAT] start', { MODEL, IS_REASONING }); },
    onText() { /* count */ },
    onError(e) { console.error('[CHAT] error', e); },
    onFinish(e) { console.log('[CHAT] finish', { reason: e.finishReason, usage: e.usage }); },
  });

  return result.toAIStreamResponse();
}
```

---

### ê²°ë¡ 

* Vercelì—ì„œ **`gptâ€‘5-mini`ëŠ” â€œì˜µì…˜ ë‹¤ì´ì–´íŠ¸â€ê°€ ê¸°ë³¸**ì…ë‹ˆë‹¤.
  *í† í° ìƒí•œ ë¯¸ì§€ì • + temperature ì œê±° + Edge + í´ë°±* ì´ 1ì°¨ ì•ˆì • í•´ë²•ì´ê³ , ì´í›„ í•„ìš” ì‹œì—ë§Œ **í”„ë¡¬í”„íŠ¸ ì œì•½** ë˜ëŠ” SDKê°€ ë³´ì¥í•˜ëŠ” ì•ˆì „í•œ í† í° ìƒí•œì„ ì‹ ì¤‘íˆ ì¶”ê°€í•˜ì„¸ìš”.
* ìœ„ ì§€ì¹¨ì€ ì—¬ëŸ¬ë¶„ì´ ì •ë¦¬í•´ ë‘” **ë§ˆì´ê·¸ë ˆì´ì…˜ ì›ì¸/ëŒ€ì‘ ë¬¸ì„œ**ì™€ ì •í™•íˆ ë¶€í•©í•©ë‹ˆë‹¤(â€œìƒí•œ íŒŒë¼ë¯¸í„°ë¡œ ì¸í•œ ë¬´ì‘ë‹µâ€).  

í•„ìš”í•˜ì‹œë©´ ì§€ê¸ˆ ë ˆí¬ êµ¬ì¡°ì— ë§ì¶˜ **ì™„ì „ íŒ¨ì¹˜ë³¸(`route.ts`)** ê³¼ **í´ë°± í¬í•¨ E2E í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**ê¹Œì§€ ë°”ë¡œ ë“œë¦´ê²Œìš”.
