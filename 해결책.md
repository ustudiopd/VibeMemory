좋아요. **VibeMemory**에서 “프로젝트별로 청크를 불러와 대화하고, 대화내용을 저장”하는 챗봇 기능을 바로 붙일 수 있게 **설계·스키마·RPC·API·프론트 연동**까지 한 번에 보실 수 있는 명세를 정리했습니다.
(기반 스택은 Next.js + Supabase(pgvector) + OpenAI(gpt‑4.1‑mini/embedding‑3‑small)이며, 기존 프로젝트의 RAG/하이브리드 검색·SSE 스트리밍·RLS 패턴을 그대로 재사용합니다.)  

---

## 0) 목표 & 사용자 경험(UX)

* **프로젝트 상세 페이지 → “Chat” 탭**에서 질문 입력
* **스코프 기본값 = 현재 프로젝트** (필요 시 “전체 프로젝트”로 확장 토글)
* 응답은 **SSE 스트리밍**으로 실시간 표시, **근거 출처(파일·청크) 카드**가 함께 붙음
* 모든 대화는 **세션 단위**로 저장되어, 좌측에서 **이전 대화 선택/검색** 가능
  (이 패턴은 기존 공고분석 챗봇 구현에서 검증됨) 

---

## 1) 아키텍처 한눈에 보기

```
[Next.js App]  ──▶  POST /api/projects/:id/chat  ──▶  [Next.js API Route]
   UI/SSE                  (SSE 응답)                           │
                                                                ▼
                                                       [RAG 서비스]
                                               ┌── FTS + pgvector(RRF) ──┐
                                               │  search_*_rrf RPC       │
                                               └─────────────┬───────────┘
                                                             ▼
                                         vibememory.repo_file_chunks (+repo_files)
                                             (프로젝트별 .md 청크 저장소)

                ┌──────── 저장 ────────┐
OpenAI (gpt-4.1-mini) ◀────────── 프롬프트(질문+컨텍스트) ──────────▶ 응답 스트림
                └── project_chat_messages / sessions 기록 ───────────┘
```

* **하이브리드 검색(RRF)**: 키워드(FTS) + 의미(pgvector)를 RRF로 결합하는 RPC를 사용합니다. 기존 프로젝트에서 이미 채택/운영 중인 전략입니다. 
* **SSE 스트리밍**: 기존 챗봇과 동일하게 SSE를 사용(또는 fetch-event-source), 메시지는 DB에 저장하고 Realtime로 동기화할 수 있습니다. 

---

## 2) 데이터 모델 (PostgreSQL / Supabase)

> **RLS 기본 원칙**: 모든 테이블은 `owner_id = auth.uid()` 기반 RLS 적용. 프로젝트와 동일한 소유자만 접근.

### 2.1 프로젝트 청크(기존)

* **repo_file_chunks**: RAG 검색 대상으로 사용, 이미 `embedding(vector 1536)`/`content`/`is_current` 등을 보유
* **repo_files**: `project_id` 보유, 청크와 조인해 **프로젝트별 필터링**에 사용
  (이 구조는 현 파이프라인 보고서/스키마에서 확인됩니다.)  

### 2.2 챗봇 세션/메시지(신규)

```sql
-- 프로젝트별 챗봇 세션
CREATE TABLE vibememory.project_chat_sessions (
  id              uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  project_id      uuid NOT NULL REFERENCES vibememory.projects(id) ON DELETE CASCADE,
  owner_id        uuid NOT NULL,               -- RLS 키
  title           text,                        -- 세션 제목(첫 사용자 메시지 요약)
  created_at      timestamptz DEFAULT now(),
  updated_at      timestamptz DEFAULT now()
);

-- 세션별 메시지
CREATE TABLE vibememory.project_chat_messages (
  id              uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id      uuid NOT NULL REFERENCES vibememory.project_chat_sessions(id) ON DELETE CASCADE,
  owner_id        uuid NOT NULL,               -- RLS 키
  role            text NOT NULL CHECK (role IN ('user','assistant','system')),
  content         text NOT NULL,
  model           text,                        -- gpt-4.1-mini 등
  token_in        int,
  token_out       int,
  latency_ms      int,
  created_at      timestamptz DEFAULT now()
);

-- 응답이 참조한 근거 청크(파일/오프셋) 기록 (선택)
CREATE TABLE vibememory.project_chat_message_citations (
  id              uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  message_id      uuid NOT NULL REFERENCES vibememory.project_chat_messages(id) ON DELETE CASCADE,
  project_id      uuid NOT NULL REFERENCES vibememory.projects(id) ON DELETE CASCADE,
  file_path       text NOT NULL,
  chunk_id        uuid,            -- repo_file_chunks.id
  score           real,            -- 유사도/가중치
  created_at      timestamptz DEFAULT now()
);

-- RLS 예시(요지)
ALTER TABLE vibememory.project_chat_sessions ENABLE ROW LEVEL SECURITY;
CREATE POLICY p_read_sessions ON vibememory.project_chat_sessions
  FOR SELECT USING (owner_id = auth.uid());
CREATE POLICY p_write_sessions ON vibememory.project_chat_sessions
  FOR INSERT WITH CHECK (owner_id = auth.uid());

ALTER TABLE vibememory.project_chat_messages ENABLE ROW LEVEL SECURITY;
CREATE POLICY p_read_msgs ON vibememory.project_chat_messages
  FOR SELECT USING (owner_id = auth.uid());
CREATE POLICY p_write_msgs ON vibememory.project_chat_messages
  FOR INSERT WITH CHECK (owner_id = auth.uid());
```

> 위 RLS/스키마 격리 패턴은 기존 시스템 전반의 보안·정책 원칙과 동일합니다.

---

## 3) 검색 RPC (프로젝트 스코프 RRF)

> 기존에 사용하던 **`hybrid_search_rrf`**(RRF) 패턴을 **프로젝트 필터**를 포함하도록 확장합니다. PostgREST 파라미터 문제로 **벡터 파라미터는 `float8[]`**로 받고 내부에서 `::vector(1536)`로 캐스팅하는 패턴을 재사용하세요.

```sql
-- 프로젝트 범위 RRF 검색
CREATE OR REPLACE FUNCTION vibememory.search_project_chunks_rrf(
  p_project_id uuid,
  p_query_text text,
  p_query_vec float8[],          -- JSON 배열로 안전히 전달
  p_limit int DEFAULT 8
)
RETURNS TABLE (
  chunk_id uuid,
  file_path text,
  content text,
  fts_rank real,
  sim real,
  rrf_score real
) LANGUAGE plpgsql SECURITY DEFINER AS $$
BEGIN
  RETURN QUERY
  WITH base AS (
    SELECT c.id AS chunk_id, rf.path AS file_path, c.content, c.embedding
    FROM vibememory.repo_file_chunks c
    JOIN vibememory.repo_files rf ON rf.id = c.repo_file_id
    WHERE rf.project_id = p_project_id AND c.is_current = true
  ),
  fts AS (
    SELECT chunk_id, content,
           ts_rank(to_tsvector('simple', content), plainto_tsquery('simple', p_query_text)) AS fts_rank
    FROM base
    WHERE to_tsvector('simple', content) @@ plainto_tsquery('simple', p_query_text)
    ORDER BY fts_rank DESC
    LIMIT p_limit
  ),
  vec AS (
    SELECT chunk_id, content,
           1 - (embedding <=> (p_query_vec::vector(1536))) AS sim
    FROM base
    ORDER BY embedding <=> (p_query_vec::vector(1536))
    LIMIT p_limit
  )
  SELECT
    coalesce(f.chunk_id, v.chunk_id) AS chunk_id,
    (SELECT b.file_path FROM base b WHERE b.chunk_id = coalesce(f.chunk_id, v.chunk_id)) AS file_path,
    coalesce(f.content, v.content) AS content,
    coalesce(f.fts_rank, 0)::real AS fts_rank,
    coalesce(v.sim, 0)::real AS sim,
    (1.0/(60+coalesce(f.fts_rank,0)) + 1.0/(60+coalesce(v.sim,0)))::real AS rrf_score
  FROM fts f
  FULL OUTER JOIN vec v ON f.chunk_id = v.chunk_id
  ORDER BY rrf_score DESC
  LIMIT p_limit;
END $$;
```

* 동일 패턴의 **기존 RRF RPC, 파라미터 타입 이슈 해결 내역**은 문서화돼 있습니다. (float8[] → `::vector(1536)`)
* 기존에 이미 노출된 **`hybrid_search_rrf`** 함수도 활용 가능(테이블/스키마 전역). 필요 시 `WHERE` 절만 프로젝트 스코프에 맞게 래핑하세요. 

---

## 4) API 명세 (Next.js App Router)

> 서버는 Next.js Route Handlers(Edge 또는 Node 런타임)로 구현. 모델은 `gpt‑4.1‑mini`, 임베딩은 `text‑embedding‑3‑small`을 사용합니다. 

### 4.1 세션

* **POST** `/api/projects/:projectId/chat/session`

  * Body: `{ title?: string }` (미지정 시 첫 user 메시지로 자동 요약)
  * Res: `{ sessionId }`
  * 동작: `project_chat_sessions`에 생성(소유자=현재 사용자). 

* **GET** `/api/projects/:projectId/chat/sessions?limit=30&cursor=...`

  * Res: 최근 세션 목록(페이징)
  * 비고: 좌측 세션 리스트에 표시

### 4.2 메시지 & 스트리밍

* **POST (SSE)** `/api/projects/:projectId/chat`

  * Body: `{ sessionId, message, topK=8 }`
  * Flow:

    1. **임베딩 생성**(`text-embedding-3-small`)
    2. `search_project_chunks_rrf(projectId, query_text, query_vec, topK)` 호출
    3. **프롬프트 조립**(질문 + 상위 K 청크 + 프로젝트 메타 요약)

       * 메타 요약은 `memory_bank` 핵심 파일(`projectbrief.md`, `techContext.md`, `systemPatterns.md`, `productContext.md`, `activeContext.md`, `progress.md`)에서 생성·캐시한 요약을 우선 사용 권장.   
    4. **OpenAI Chat Completions**(gpt‑4.1‑mini) **스트리밍**
    5. 토큰이 들어올 때마다 SSE로 전달(`event: token`), 종료 시 `event: done`
    6. **메시지 저장**: `project_chat_messages`(user/assistant) + `project_chat_message_citations`(근거)
       (이 흐름과 저장 패턴은 기존 공고분석 챗봇에서 이미 검증됨) 

* **GET** `/api/projects/:projectId/chat/messages?sessionId=...`

  * Res: 메시지 히스토리(최신 n개 + 이전 더보기 페이징)

> *팁*: SSE는 **fetch + ReadableStream** 또는 **@microsoft/fetch‑event‑source**로 수신(POST 지원), 서버는 표준 SSE 포맷으로 전송합니다. 기존 구현에서 스트리밍과 저장/동기화 UX 패턴을 그대로 재사용하세요. 

---

## 5) 프론트엔드 UX 컴포넌트

* **ProjectChatPanel**

  * 상단: 프로젝트 스코프 표시(“이 프로젝트의 .md만 검색”)
  * 중앙: 메시지 리스트(스트리밍 중 상태 표시), 근거 카드(파일/청크, 유사도 점수)
  * 하단: 입력창, 전송/중지 버튼
* **SessionSidebar**

  * 세션 목록(최근순), 검색, 새 세션 만들기
* **파일 근거 카드**

  * *`file_path`* + *snippet* + *유사도/rrf_score* 표시, 클릭 시 해당 문서 미리보기

> UI 라이브러리는 Tailwind + Shadcn/ui를 사용 중이며, 진행률/시각화 등 다른 페이지에서도 일관된 UI 패턴을 쓰고 있습니다. 

---

## 6) 프롬프트 엔지니어링(요지)

* **역할**: “이 프로젝트의 문서(.md)만 근거로 삼는 기술 비서”
* **지시**:

  * “반드시 ‘근거 청크’에서 인용하여 답하세요. 출처 파일/청크를 함께 반환하세요.”
  * “근거가 불충분하면 추론하지 말고 ‘근거 부족’으로 답변하세요.”
  * “질문이 광범위할 경우, 프로젝트 메타 요약(프로젝트 요약/아이디어/기술/특허 분석 요지)을 먼저 개요로 제시하세요.”
* **컨텍스트 구성**:

  * Top‑K 청크(검색 결과)
  * 프로젝트 메타 요약(필요 시) — 메타는 `project_analysis`가 쌓이면 그것도 활용 가능. 

---

## 7) 운영 고려사항

1. **토큰/비용 관리**

   * 히스토리는 **세션 요약 메시지(system)**로 압축(임계치 초과 시 자동 요약)
   * **Top‑K=8 기본**, 유사도 낮은 청크는 제외(문턱값)
   * 모델/토큰/지연시간은 메시지에 기록해 비용·성능 모니터링

2. **성능**

   * RRF RPC는 FTS GIN + HNSW 인덱스가 전제(마이그레이션에 포함) 
   * PostgREST 파라미터 이슈는 **float8[] → ::vector(1536)** 캐스팅으로 회피(이미 검증)

3. **보안/RLS**

   * 모든 테이블에 RLS 강제, API에서 **프로젝트 소유자 검증** 선행

4. **실시간 동기화(선택)**

   * `project_chat_messages`에 Supabase Realtime을 붙이면 다중 창/사용자 동시 보기 가능(기존 구현 경험 있음) 

---

## 8) 마이그레이션/적용 순서 (체크리스트)

1. **DB**: `project_chat_sessions / messages / message_citations` 테이블 + RLS 정책 생성
   (스키마 = `vibememory`)
2. **RPC**: `search_project_chunks_rrf` 함수 생성(또는 기존 `hybrid_search_rrf` 래핑) 
3. **API**:

   * `POST /api/projects/:id/chat/session`
   * `GET /api/projects/:id/chat/sessions`
   * `POST /api/projects/:id/chat` (SSE)
   * `GET /api/projects/:id/chat/messages`
4. **프론트**: **Chat 탭** + **Session 사이드바** + **근거 카드** 컴포넌트
5. **프롬프트**: “근거 강제·출처 표기·요약 주의” 지침 반영
6. **검증**:

   * 단위: RPC 점검(FTS/pgvector/HNSW)
   * 통합: SSE 스트리밍 & 저장 & 재로딩
   * E2E: “프로젝트 요약?”, “특정 파일의 결정사항?” 등 시나리오

---

## 9) 참조/재사용 포인트

* **기술 스택/환경**: Next.js App Router, Supabase(pgvector), OpenAI(gpt‑4.1‑mini/embedding‑3‑small)  
* **프로젝트 파이프라인의 RAG 자산**: `repo_files`/`repo_file_chunks`에 저장된 **모든 .md 청크**를 검색 대상으로 사용
* **하이브리드 검색 RPC & 파라미터 전략**: `hybrid_search_rrf` + `float8[]` 파라미터 캐스팅 방식 재사용 
* **SSE/세션·메시지 저장·근거 표기 패턴**: 기존 챗봇 구현 사례에서 동일 UX/아키텍처를 적용 

---

### 부록 A. 모델·임베딩 설정 (권장 고정값)

* **생성모델**: `gpt-4.1-mini` (빠르고 경제적)
* **임베딩**: `text-embedding-3-small` (1536차원)
* **청킹**: 1,000자, 오버랩 200자 (이미 운용 중)

### 부록 B. 프로젝트 메타 요약 소스(메모리뱅크)

* `projectbrief.md`, `techContext.md`, `systemPatterns.md`, `productContext.md`, `activeContext.md`, `progress.md`
  (챗봇 상단 개요/정합성 유지에 활용) 

---

원하시면 위 **SQL/RPC/Route 핸들러 스켈레톤**을 코드로도 바로 풀어드릴게요. 우선은 현재 레포의 구조/정책을 그대로 이어받아 **리스크 없이** 붙일 수 있게 설계만 정리했습니다.
